<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Project on Mix Lab</title><link>https://mixlabweb.github.io/categories/project/</link><description>Recent content in Project on Mix Lab</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 13 Dec 2022 11:15:33 -0700</lastBuildDate><atom:link href="https://mixlabweb.github.io/categories/project/index.xml" rel="self" type="application/rss+xml"/><item><title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models</title><link>https://mixlabweb.github.io/post/2022/12/13/harvesting-symbolic-knowledge-graphs-from-neural-pretrained-language-models/</link><pubDate>Tue, 13 Dec 2022 11:15:33 -0700</pubDate><guid>https://mixlabweb.github.io/post/2022/12/13/harvesting-symbolic-knowledge-graphs-from-neural-pretrained-language-models/</guid><description>Knowledge graphs (KGs) encode rich knowledge about entities and their relationships, and have been one of the major means for organizing commonsense or domain-specific information to empower various applications, including search engines, recommendation systems, chatbots, healthcare, etc. Traditionally, a KG is constructed by expensive human crowdsourcing (WordNet, ConceptNet, ATOMIC…). Researchers have also explored automatic knowledge graph construction with text mining techniques, but it’s still a quite heavy task due to the large corpus and complex pipelines.</description></item></channel></rss>