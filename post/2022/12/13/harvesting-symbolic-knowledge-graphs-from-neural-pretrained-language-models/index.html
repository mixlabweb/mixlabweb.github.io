<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models | Mix Lab</title><meta name=description content="Mix Lab: Machine Intelligence with all eXperience"><link rel=stylesheet href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-R15X7YW1Z3"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R15X7YW1Z3",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-R15X7YW1Z3","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><script>const theme=localStorage.getItem("theme");document.body.classList.add(theme||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark-theme":"light-theme"))</script><nav><div class=nav><div class=header-logo><a href=https://mixlabweb.github.io class=header-logo-text>Mix Lab</a><div class=separator>|</div><div class=header-item><i class=header-theme-switcher id=switch-theme><svg id="switch-theme-light" class="header-theme-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor"><path fill-rule="evenodd" d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" clip-rule="evenodd"/></svg><svg id="switch-theme-dark" class="header-theme-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg></i></div></div><ul class=header-items-container><li class=header-item><a class=header-link href=/about/>About</a></li><li class=header-item><a class=header-link href=/post/>Posts</a></li><li class=header-item><a class=header-link href=/categories/>Categories</a></li><li class=header-item><a class=header-link href=/tags/>Tags</a></li></ul></div></nav><div class=content><div class=post-meta><h1><span class=page-title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models</span></h1><div class=post-meta-details><span class=post-author>Shibo Hao</span>
<span class=post-date>2022/12/13</span><ul class=post-tags><li><a href=https://mixlabweb.github.io/tags/project>Project</a></li></ul><ul class=post-tags><li><a href=https://mixlabweb.github.io/tags/knowledge-graph>Knowledge Graph</a></li><li><a href=https://mixlabweb.github.io/tags/language-models>Language Models</a></li></ul></div></div><aside class=toc></aside><article class=markdown><p><img src=/bertnet_1.png style=width:5.46354in;height:2.97754in></p><p>Knowledge graphs (KGs) encode rich knowledge about entities and their relationships, and have been one of the major means for organizing commonsense or domain-specific information to empower various applications, including search engines, recommendation systems, chatbots, healthcare, etc. Traditionally, a KG is constructed by expensive human crowdsourcing (WordNet, ConceptNet, ATOMIC…). Researchers have also explored automatic knowledge graph construction with text mining techniques, but it’s still a quite heavy task due to the large corpus and complex pipelines. Besides, the involved relations are limited to only those covered by the selected corpus. In reality, people sometimes will not explicitly express knowledge in their language, because they are thought to be “common sense”. Can we think of a method for knowledge graph construction that is both fully automatic and flexible enough to capture any relations?</p><p>In this work, we ask language models for help. For the past few years, we have seen the rapid growth of language models (LMs) these years (e.g. BERT, ROBERTA, and GPT-3). They have been shown to encode a large amount of implicit knowledge inside their parameters, which inspired the interest in using the LMs as knowledge bases (LAMA [1]). For example, one can query "Obama was born in " to a LM to get the answer "Hawaii". However, we are not satisfied with this usage because the black-box LMs, where knowledge is only implicitly encoded, fall short of the many nice properties of explicit KGs, such as the easiness of browsing the knowledge or even making updates and the explainability for trustworthy use by domain experts. Our work makes the first attempt to <strong>automatically harvest KGs from the LMs</strong>, with the goal to combine the best of both worlds, namely the flexibility and scalability from the neural LMs, and the access, editability, and explainability in the symbolic form.</p><h1 id=bertnet-framework>BertNet framework</h1><p>We first formulate the problem: given a description of a relation, we want to harvest entity tuples of this relation from a language model. Here, A relation is described as a prompt with entity slots. The relation is further disambiguated with a handful of example seed entity tuples. With these inputs, our framework is expected to output a list of entity tuples with confidence. (See Figure 1)</p><h2 id=compatibility-score>Compatibility Score</h2><p>Before we dive into the two main stages of our framework, we introduce the compatibility score between a prompt and an entity tuple.</p><p><span class="math display">\[
f_{L M}(\langle h, t\rangle, p)=\\\alpha \log P_{L M}(h, t \mid p)+(1-\alpha) \min \left\{\log P_{L M}(h \mid p), \log P_{L M}(t \mid p, h)\right\}
\]</span></p><p>With Bert as an example, the left term is the probability of filling the entity tuple &lt;h, t> into the slots in the prompt p. Typically, this joint conditional probability can be decomposed to $P_{LM}(h|p) \times P_{LM}(t|h,p)$, which means it’s computed in autoregressive style. Besides, we also want to make sure that the probability of each step shouldn’t be too low, and that’s the intuition behind the second term. A concrete example is shown in Figure 2, where p=“A is the place for B”, h=“library” and t=“study”. We also present how to process multi-token entities where h=“study room”.</p><p><img src=/bertnet_2.png style=width:6.5in;height:2.65278in></p><p>With this compatibility score, we then introduce the workflow of harvesting knowledge graphs from language models (Figure 3), which can be divided into two main stages: Prompt Creation and Entity Tuple Search.</p><h2 id=stage-1-prompt-creation>Stage 1: Prompt Creation</h2><p><img src=/bertnet_3.png style=width:6.5in;height:3.19444in></p><p>A known deficiency of language models is their inconsistency when given different prompts. Sometimes even a slight difference in wording would cause a drastic change in prediction [2]. To this end, we want to generate multiple paraphrases of the input prompt, and use them to regularize the output of language models.</p><p>We iteratively sample entity tuples and prompts to assemble a statement and paraphrase it (Here we call the API of GPT-3). The process is shown in the left part of Figure 3. The generated prompts can be semantically drifted, so we weigh them by averaging the compatibility score between a prompt and each given seed entity tuple. The weights are further normalized with softmax across all prompts. The resulting weighted prompt set serves as a more reliable description of the relation.</p><h2 id=stage-2-entity-tuple-search>Stage 2: Entity Tuple Search</h2><p>Our goal in the following stage is to search for entity tuples that maximize compatibility with the weighted prompt set.</p><p><span class="math display">\[
\text{consistency} \left(\left\langle h^{\text {new }}, t^{\text {new }}\right\rangle\right)=\sum_p w_p \cdot f_{L M}\left(\left\langle h^{\text {new }}, t^{\text {new }}\right\rangle, p\right)
\]</span></p><p>Since the searching space is too large, we propose an approximation, that is to only use the minimum individual log-likelihoods (the left part of compatibility score, shortened as MLL) instead of the full equation. This allows us to apply an efficient pruning strategy.</p><p>As a running example, when we are searching for 100 entity tuples, we maintain a minimum heap to keep track of the MLL of the existing entity pair set. The maximum size of this heap is 100, and the heap top can be used as a threshold for future search because it’s the 100-th largest MLL: When we are searching for a new entity tuple, once we find the log-likelihood at any time step is lower than the threshold, we can prune the continuous searching immediately because this means the MLL of this tuple will never surpass any existing tuples in the heap. If a new entity tuple is searched out without being pruned, we will pop the heap and push the MLL of the new tuple. Intuitively, the pruning process makes sure that the generated part of the tuple in searching is reasonable for the given prompt.</p><p>Once we collect a large number of proposed entity tuples, we re-rank them with the full compatibility score. We finally use various thresholds to get the outcome KGs in different scales, including (1) 50%: taking half of all searched-out entities with higher scores. (2) base-k: Naturally there are different numbers of valid tuples for different relations (e.g. tuples of CAPITAL_OF should not exceed 200 as that is the number of all the countries in the world). We design a relation-specific thresholding method, that is to set 10% of the k-nd score as the threshold (i.e., 0.1 ∗ score_k), and retain all tuples with scores above the threshold. We name the settings base-10 and base-100 when k is 10 and 100, respectively.</p><h1 id=outcome-kgs>Outcome KGs</h1><p>Different from traditional KGs, BertNet is extensible every time there is a new query. In essence, there is no limitation on the scalability of BertNet. In our evaluation, we ground our framework to a commonly used relation set from ConceptNet, and a New relation set composed of some novel relations the authors think of.</p><p><span class="math display">\[
\begin{array}{lllcc}
\hline \text { Method } & \text { Tuple } & \text { Diversity } & \text { Novelty\% } & \text { Acc\% } \\
\hline \text { WebChild } & 4,649,471 & - & - & 82.0^* \\
\text { ASCENT } & 8,600,000 & - & - & 79.2^* \\
\text { TransOMCS } & 18,481,607 & 100,659 & 98.3 & 56.0^* \\
\hline \text { COMET }_{\text {base-10 }}^{C N} & 6,741 & 4,342 & 35.5 & 92.0 \\
\text { COMET }_{50 \%}^{C N} & 230,028 & 55,350 & 72.4 & 66.6 \\
\hline \text { ROBERTANET }_{\text {base-10 }}^{C N} & 6,741 & 6,107 & 64.4 & 88.0 \\
\text { RoBERTANET }_{\text {base-100 }}^{C N} & 24,375 & 12,762 & 68.8 & 81.6 \\
\text { ROBERTANET }_{50 \%}^{C N} & 230,028 & 80,525 & 87.0 & 55.0 \\
\hline \text { RoBERTANET T }_{\text {base-10 }}^{\text {New }} & 2,180 & 3,137 & - & 81.8 \\
\text { RoBERTANET }_{\text {base-100 }}^{\text {New }} & 7,329 & 6,559 & - & 68.6 \\
\text { ROBERTANET }_{50 \%}^{\text {New }} & 23,666 & 16,089 & - & 58.6 \\
\hline
\end{array}
\]</span></p><p>Using solely the LM as the knowledge source, and without any training data, our framework extracts KGs with high accuracy and diversity. (Other methods build knowledge graphs in different settings, and thus are not comparable to ours) We also show the trade-off between scale and accuracy by using different thresholds.</p><h1 id=analysis-of-our-framework>Analysis of our framework</h1><p>We did another set of experiments to evaluate our prompt creation method. Looking at the top-100 tuples of New relations harvested with different prompts, we find that the paraphrasing-based method largely outperformed the previous few-shot learning method Autoprompt [3]. We also show that the ensemble of multiple prompts is better than only using human-written prompts or the top-1 prompts.</p><p>We also harvest KGs from 5 different LMs and evaluate them in the same setting. The results shed some new light on several knowledge-related questions regarding the LMs’ knowledge capacity:</p><p><span class="math display">\[
\begin{array}{rcc}
\hline \text { Methods } & \text { Acc } & \text { Rej } \\
\hline \text { AUTOPROMPT } & 0.33 & 0.47 \\
\text { HUMAN PROMPT } & 0.60 & 0.27 \\
\text { TOP-1 PROMPT (Ours) } & 0.69 & 0.23 \\
\text { MULTI PROMPTS (Ours) } & \mathbf{0 . 7 3} & \mathbf{0 . 2 0} \\
\hline
\end{array}\\
\]</span> <span class="math display">\[
\begin{array}{rcc}
\hline \text { Source LMs } & \text { Acc } & \text { Rej } \\
\hline \text { DISTILBERT } & 0.67 & 0.24 \\
\text { BERT-BASE } & 0.63 & 0.26 \\
\text { BERT-LARGE } & 0.70 & 0.22 \\
\text { ROBERTA-BASE } & 0.70 & 0.22 \\
\text { ROBERTA-LARGE } & 0.73 & 0.20 \\
\hline
\end{array}
\]</span></p><p><strong>Does a larger LM encode better knowledge?</strong></p><p>For BERT (and RoBERTa), the large version and the base version share the same pretraining corpus and tasks, respectively, while the large version has a larger model architecture than the base version in terms of layers (24 v.s. 12), attention heads (16 v.s. 12), and the number of parameters (340M v.s. 110M). We can see that BertNet-large and RoBERTaNet-large are around 7% and 3% higher than their base version, separately, so the large models indeed encoded better knowledge than the base models.</p><p><strong>Does better pretraining bring better knowledge?</strong></p><p>RoBERTa uses the same architecture as BERT but with better pretraining strategies, like dynamic masking, larger batch size, etc. In their extracted knowledge graphs from our framework, RoBERTaNet-large performs better than BertNet-large (0.73 v.s. 0.70), and RoBERTaNet-base is also better than BertNet-base (0.70 v.s. 0.63), which indicates the better pretraining indeed bring the better knowledge learning and storage.</p><p><strong>Is knowledge really kept in the knowledge distillation process?</strong></p><p>DistilBERT is trained by distilling BERT-base, and reduces 40% parameters from it. Interestingly, the knowledge distillation process instead improves by around 4% of accuracy in the result knowledge graph. This might be because the knowledge distillation is able to remove some noisy information from the teacher model.</p><h1 id=summary>Summary</h1><p>In this work, we propose an automatic framework to extract a KG with arbitrary relations in an efficient and scalable way, and we apply this framework to harvest KGs from a wide range of popular LMs, indicating LMs alone can be good resources for KG construction. Our framework shows a new style beyond traditional work, and provides a fully symbolic interpretation to the LM, leading to new insights into the knowledge capability of LMs.</p><h1 id=reference>Reference</h1><p>[1] Language Models as Knowledge Bases?, EMNLP 2019</p><p>[2] Measuring and improving consistency in pretrained language models, TACL 2021</p><p>[3] Eliciting knowledge from language models using automatically generated prompts, EMNLP 2020</p></article></div><footer class=markdown>© <a href=mixlabweb.github.io>Mix Lab</a> 2019 – 2022 | <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC BY-NC 4.0</a> | <a href=index.xml>Subscribe</a></footer></body><script>const switchTheme=document.getElementById("switch-theme");switchTheme.addEventListener("click",()=>{document.body.classList.toggle("dark-theme"),document.body.classList.toggle("light-theme"),document.body.classList.contains("dark-theme")?localStorage.setItem("theme","dark-theme"):localStorage.setItem("theme","light-theme")}),tocbot.init({tocSelector:".toc",contentSelector:".markdown",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></html>