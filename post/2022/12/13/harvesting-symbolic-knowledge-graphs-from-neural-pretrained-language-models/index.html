<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models | Mix Lab</title><meta name=description content="Mix Lab: Machine Intelligence with all eXperience"><link rel=stylesheet href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-R15X7YW1Z3"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R15X7YW1Z3",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-R15X7YW1Z3","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><script>const theme=localStorage.getItem("theme");document.body.classList.add(theme||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark-theme":"light-theme"))</script><nav><div class=nav><div class=header-logo><a href=https://mixlabweb.github.io class=header-logo-text>Mix Lab</a><div class=separator>|</div><div class=header-item><i class=header-theme-switcher id=switch-theme><svg id="switch-theme-light" class="header-theme-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor"><path fill-rule="evenodd" d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" clip-rule="evenodd"/></svg><svg id="switch-theme-dark" class="header-theme-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg></i></div></div><ul class=header-items-container><li class=header-item><a class=header-link href=/about/>About</a></li><li class=header-item><a class=header-link href=/post/>Posts</a></li><li class=header-item><a class=header-link href=/categories/>Categories</a></li><li class=header-item><a class=header-link href=/tags/>Tags</a></li></ul></div></nav><div class=content><div class=post-meta><h1><span class=page-title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models</span></h1><div class=post-meta-details><span class=post-author>Shibo Hao</span>
<span class=post-date>2022/12/13</span><ul class=post-tags><li><a href=https://mixlabweb.github.io/categories/project>Project</a></li></ul><ul class=post-tags><li><a href=https://mixlabweb.github.io/tags/knowledge-graph>Knowledge Graph</a></li><li><a href=https://mixlabweb.github.io/tags/language-models>Language Models</a></li></ul></div></div><aside class=toc></aside><article class=markdown><p><img src=/bertnet_1.png style=width:5.46354in;height:2.97754in></p><p>Knowledge graphs (KGs) are a useful tool for representing rich symbolic knowledge about entities and their relationships, and have been applied in a variety of fields, including search engines, recommendation systems, chatbots, and healthcare. Traditionally, KGs have been constructed through expensive human crowdsourcing efforts (such as <a href=https://wordnet.princeton.edu/><span class=underline>WordNet</span></a>, <a href=https://conceptnet.io/><span class=underline>ConceptNet</span></a>, and <a href=https://allenai.org/data/atomic-2020><span class=underline>ATOMIC</span></a>). Researchers have also explored the use of text mining techniques for automatic KG construction, but this remains a challenging task due to large corpus and complex processing required. Besides, an inevitable shortcoming of text mining is that the relations are limited to those covered by the selected corpus. For example, much of the commonsense knowledge would not be explicitly expressed in human languages, making it difficult to extract them from corpus. How to build a knowledge graph of <strong>any relations</strong> is still an underexplored topic.</p><p>There has been a significant increase in the number of deep neural models that have achieved high levels of performance on various tasks across diverse domains, such as language modeling with GPT-3 and ChatGPT, and medical prediction with bioBERT. During the training, these models can store the knowledge learned from data implicitly in their parameters. For instance, a medical model trained on large-scale health records may have acquired a wealth of medical knowledge, allowing it to accurately predict diseases. Similarly, a pandemic prediction model with accurate trend simulation may have implicitly captured certain transmission patterns from the data it was trained on.</p><p>Recent research attempted to utilize these LMs as knowledge bases (LAMA). For example, with manually or automatically crafted prompts (e.g., "Obama was born in __") to query the LMs for answers (e.g., "Hawaii") However, the black-box LMs, where knowledge is only implicitly encoded, fall short of the many nice properties of symbolic KGs, such as the ease of browsing the knowledge or even making updates. This leads to the question: can we automatically harvest KGs from the LMs, and hence combine the best of both worlds, namely the flexibility and scalability from the neural LMs, and the access, editability, and explainability in the symbolic form?</p><p>This work represents a step towards achieving this goal. Our automatic framework is able to efficiently and scalably extract a knowledge graph (KG) from a pretrained language model (LM) such as BERT or RoBerta, resulting in a family of new KGs (e.g., BertNet, RoBertaNet) that provide a broader and extendable set of relations and entities beyond those in existing hand-annotated KGs like ConceptNet.</p><p>In the following, we’ll briefly present our framework. Please refer to our <a href=https://arxiv.org/pdf/2206.14268.pdf><span class=underline>paper</span></a> for more details. We have released our code and the outcome KGs from on <a href=https://github.com/tanyuqian/knowledge-harvest-from-lms/><span class=underline>Github</span></a>, and you are also welcome to try out our <a href=http://lmnet.io><span class=underline>knowledge server demo</span></a>! (Figure 1)</p><figure><img src=/bertnet_demo_screenshot.png style=width:6.5in;height:5.70833in alt><figcaption>1: A demo of our knowledge server</figcaption></figure><h2 id=our-framework-to-harvest-kgs-from-lms>Our framework to harvest KGs from LMs</h2><p>We first formulate the problem: given a description of a relation, we want to harvest entity tuples of this relation from a language model. Here, a relation is framed as a prompt with entity slots which is further disambiguated with a handful of example seed entity tuples. With these inputs, our framework is expected to output a list of entity tuples with confidence (Figure 2).</p><h3 id=compatibility-score>Compatibility Score</h3><p>Before we dive into the two main stages of our framework, we introduce the compatibility score between a prompt and an entity tuple.</p><p><span class="math display">\[
f_{L M}(\langle h, t\rangle, p)=\\\alpha \log P_{L M}(h, t \mid p)+(1-\alpha) \min \left\{\log P_{L M}(h \mid p), \log P_{L M}(t \mid p, h)\right\}
\]</span></p><p>With BERT as an example, the first term in the scoring function involves the probability of filling the entity tuple $&lt;h, t>$ into the slots in the prompt $p$. Typically, this joint probability of entity pairs can be decomposed to $P_{LM}(h|p) \times P_{LM}(t|h,p)$, assuming it’s computed in an autoregressive style. Besides, we also want to make sure that the probability of each step shouldn’t be too low, and that’s the intuition behind the second term. A concrete example is shown in Figure 1, where p=“A is the place for B”, h=“library” and t=“study”. We also present how to process multi-token entities where h=“study room”.</p><figure><img src=/bertnet_2.png style=width:6.5in;height:2.65278in alt><figcaption>2: A running example of computing compatibility score</figcaption></figure><p>With this compatibility score, we then introduce the workflow of harvesting knowledge graphs from language models (Figure 2), which can be divided into two main stages: Prompt Creation and Entity Tuple Search.</p><h3 id=stage-1-prompt-creation>Stage 1: Prompt Creation</h3><figure><img src=/bertnet_3.png style=width:6.5in;height:3.19444in alt><figcaption>3: Overview of our framework</figcaption></figure><p>A known deficiency of language models is their inconsistency when given different prompts. Sometimes even a slight difference in wording would cause a drastic change in the prediction results. To this end, we want to generate multiple paraphrases of the initial input prompt, and use them to regularize the output of language models.</p><p>As our implementation, the algorithm iteratively samples entity tuples and prompts to assemble a statement and paraphrase it (specifically, usingGPT-3 API). The process is shown in the left part of Figure 2. The generated prompts can be semantically drifted, so the prompts are weighted by the average compatibility scores between a prompt and all given seed entity tuples. The weights are further normalized with softmax across all prompts. The resulting weighted prompt set serves as a more reliable description of the relation.</p><h3 id=stage-2-entity-tuple-search>Stage 2: Entity Tuple Search</h3><p>Our goal in the following stage is to search for entity tuples that achieves high compatibility with the weighted prompt set.</p><p><span class="math display">\[
\text{consistency} \left(\left\langle h^{\text {new }}, t^{\text {new }}\right\rangle\right)=\sum_p w_p \cdot f_{L M}\left(\left\langle h^{\text {new }}, t^{\text {new }}\right\rangle, p\right)
\]</span></p><p>Since the entity search space is too large, we propose an approximation that only uses the minimum individual log-likelihoods (the left part of compatibility score, shortened as MLL) instead of the full equation. This cheaper scoring function allows fast rollouts by pruning.</p><p>As a running example, when we are searching for 100 entity tuples, we maintain a minimum heap to keep track of the MLL of the existing entity pair set. The maximum size of this heap is 100, and the heap top can be used as a threshold for future search because it’s the 100-th largest MLL: When we are searching for a new entity tuple, once we find the log-likelihood at any timestamp is lower than the threshold, we can prune the continuous searching immediately because this means the MLL of this tuple will never surpass any existing tuples in the heap. If a new entity tuple is reached without being pruned, we pop the heap and push the MLL of the new tuple.</p><p>Once we collect a large number of potential entity tuples, we re-rank them with the full compatibility score (as the confidence for them). We finally use various thresholds to get the outcome KGs in different scales, including (1) 50%: taking half of all searched-out entities with higher scores. (2) base-k: Naturally there are different numbers of valid tuples for different relations (e.g. tuples of CAPITAL_OF should not exceed 200 as that is the number of all the countries in the world). We design a relation-specific thresholding method, that is to set 10% of the k-nd score as the threshold (i.e., 0.1 ∗ score_k), and retain all tuples with scores above the threshold. We name the settings base-10 and base-100 when k is 10 and 100, respectively.</p><h2 id=outcome-kgs>Outcome KGs</h2><p>Different from traditional KGs, BertNet is extensible in case a new query is desired.. In essence, there is no limitation on the scalability of BertNet. In our evaluation, we ground our framework to a commonly used relation set from ConceptNet, and a New relation set composed of some novel relations created by the authors (e.g. “capable but not good at”).</p><p><span class="math display">\[
\begin{array}{lllcc}
\hline \text { Method } & \text { Tuple } & \text { Diversity } & \text { Novelty\% } & \text { Acc\% } \\
\hline \text { WebChild } & 4,649,471 & - & - & 82.0^* \\
\text { ASCENT } & 8,600,000 & - & - & 79.2^* \\
\text { TransOMCS } & 18,481,607 & 100,659 & 98.3 & 56.0^* \\
\hline \text { COMET }_{\text {base-10 }}^{C N} & 6,741 & 4,342 & 35.5 & 92.0 \\
\text { COMET }_{50 \%}^{C N} & 230,028 & 55,350 & 72.4 & 66.6 \\
\hline \text { ROBERTANET }_{\text {base-10 }}^{C N} & 6,741 & 6,107 & 64.4 & 88.0 \\
\text { RoBERTANET }_{\text {base-100 }}^{C N} & 24,375 & 12,762 & 68.8 & 81.6 \\
\text { ROBERTANET }_{50 \%}^{C N} & 230,028 & 80,525 & 87.0 & 55.0 \\
\hline \text { RoBERTANET T }_{\text {base-10 }}^{\text {New }} & 2,180 & 3,137 & - & 81.8 \\
\text { RoBERTANET }_{\text {base-100 }}^{\text {New }} & 7,329 & 6,559 & - & 68.6 \\
\text { ROBERTANET }_{50 \%}^{\text {New }} & 23,666 & 16,089 & - & 58.6 \\
\hline
\end{array}
\]</span> Table 1: Statistics of different knowledge graphs</p><p>Using solely the LM as the knowledge source, and without any training data, our framework extracts KGs with high accuracy and diversity. (Other methods in the table build knowledge graphs in different settings, and thus are not comparable to ours) We also show the trade-off between scale and accuracy by using different thresholds.</p><h2 id=summary>Summary</h2><p>In this work, we propose an automatic framework for extracting a knowledge graph from language models with arbitrary relations in an efficient and scalable manner, and demonstrate its application in creating KGs of two sets of relations. Our results indicate that LMs can be effective resources for KG construction on their own. Our framework also offers a fully symbolic interpretation of the LM, providing new insights into its knowledge capabilities.</p></article></div><footer class=markdown>© <a href=mixlabweb.github.io>Mix Lab</a> 2019 – 2022 | <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC BY-NC 4.0</a> | <a href=index.xml>Subscribe</a></footer></body><script>const switchTheme=document.getElementById("switch-theme");switchTheme.addEventListener("click",()=>{document.body.classList.toggle("dark-theme"),document.body.classList.toggle("light-theme"),document.body.classList.contains("dark-theme")?localStorage.setItem("theme","dark-theme"):localStorage.setItem("theme","light-theme")}),tocbot.init({tocSelector:".toc",contentSelector:".markdown",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></html>