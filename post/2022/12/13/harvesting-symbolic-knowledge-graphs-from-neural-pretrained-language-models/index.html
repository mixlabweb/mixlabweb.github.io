<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models | Mix Lab</title><meta name=description content="Mix Lab: Machine Intelligence with all eXperience"><link rel=stylesheet href=/css/style.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Code&family=Source+Sans+Pro:ital,wght@0,200;0,300;0,400;0,600;0,700;0,900;1,200;1,300;1,400;1,600;1,700;1,900&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css integrity=sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js integrity=sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/tocbot/4.11.1/tocbot.min.js></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-R15X7YW1Z3"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-R15X7YW1Z3",{anonymize_ip:!1})}</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","G-R15X7YW1Z3","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body><script>const theme=localStorage.getItem("theme");document.body.classList.add(theme||(window.matchMedia("(prefers-color-scheme: dark)").matches?"dark-theme":"light-theme"))</script><nav><div class=nav><div class=header-logo><a href=https://mixlabweb.github.io class=header-logo-text>Mix Lab</a><div class=separator>|</div><div class=header-item><i class=header-theme-switcher id=switch-theme><svg id="switch-theme-light" class="header-theme-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor"><path fill-rule="evenodd" d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" clip-rule="evenodd"/></svg><svg id="switch-theme-dark" class="header-theme-icon" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentcolor"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg></i></div></div><ul class=header-items-container><li class=header-item><a class=header-link href=/about/>About</a></li><li class=header-item><a class=header-link href=/post/>Posts</a></li><li class=header-item><a class=header-link href=/categories/>Categories</a></li><li class=header-item><a class=header-link href=/tags/>Tags</a></li></ul></div></nav><div class=content><div class=post-meta><h1><span class=page-title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models</span></h1><div class=post-meta-details><span class=post-author>Shibo Hao</span>
<span class=post-date>2022/12/13</span><ul class=post-tags><li><a href=https://mixlabweb.github.io/categories/project>Project</a></li></ul><ul class=post-tags><li><a href=https://mixlabweb.github.io/tags/knowledge-graph>Knowledge Graph</a></li><li><a href=https://mixlabweb.github.io/tags/language-models>Language Models</a></li></ul></div></div><aside class=toc></aside><article class=markdown><p><img src=/bertnet_1.png style=width:5.46354in;height:2.97754in></p><p>There has been a significant increase in the number of deep neural models that have achieved high levels of performance on various tasks across diverse domains, such as language generation with GPT-3 and ChatGPT, and medical prediction with bioBERT. These models have the ability to function as aggregators of information and organizers of diverse data examples and experiences due to the vast amount of knowledge and information that has been implicitly encoded within their parameters. For instance, a medical model trained on large-scale health records may have acquired a wealth of medical knowledge, allowing it to accurately predict diseases. Similarly, a pandemic prediction model with accurate trend simulation may have implicitly captured certain transmission patterns from the data it was trained on.</p><p>Knowledge graphs (KGs) are a useful tool for organizing and encoding rich symbolic knowledge about entities and their relationships, and have been applied in a variety of fields, including search engines, recommendation systems, chatbots, and healthcare. Traditionally, KGs have been constructed through expensive human crowdsourcing efforts (such as WordNet, ConceptNet, and ATOMIC). Researchers have also explored the use of text mining techniques for automatic KG construction, but this remains a challenging task due to the large corpus and complex processing required. Additionally, the relations captured in these KGs are limited to those covered by the selected corpus.</p><p>On the other hand, large language models (LMs) such as BERT, RoBerta, and GPT-3, which are pretrained on massive text corpora, have demonstrated the ability to encode a significant amount of knowledge implicitly in their parameters. This has sparked interest in utilizing these LMs as knowledge bases, leading to the question: can we automatically harvest KGs from the LMs, and hence combine the best of both worlds, namely the flexibility and scalability from the neural LMs, and the access, editability, and explainability in the symbolic form?</p><p>This work represents a step towards achieving this goal. Our automatic framework is able to efficiently and scalably extract a knowledge graph (KG) from a pretrained language model (LM) such as BERT or RoBerta, resulting in a family of new KGs (e.g., BertNet, RoBertaNet) that provide a broader and extendable set of relations and entities beyond those found in existing hand-annotated KGs like ConceptNet. This allows for the inclusion of a new and expandable set of knowledge.</p><p>In the following, we’ll briefly present our framework. Please refer to our <a href=https://arxiv.org/pdf/2206.14268.pdf><span class=underline>paper</span></a> for more details. We have released our code and the outcome KGs from on <a href=https://github.com/tanyuqian/knowledge-harvest-from-lms/><span class=underline>Github</span></a>, and we also encourage everyone to try out our <a href=http://lmnet.io><span class=underline>knowledge server demo</span></a>!</p><h3 id=our-framework-to-harvest-kgs-from-lms>Our framework to harvest KGs from LMs</h3><p>We first formulate the problem: given a description of a relation, we want to harvest entity tuples of this relation from a language model. Here, a relation is framed as a prompt with entity slots which is furtherdisambiguated with a handful of example seed entity tuples. With these inputs, our framework is expected to output a list of entity tuples with confidence (Figure 1).</p><h4 id=compatibility-score>Compatibility Score</h4><p>Before we dive into the two main stages of our framework, we introduce the compatibility score between a prompt and an entity tuple.</p><p><span class="math display">\[
f_{L M}(\langle h, t\rangle, p)=\\\alpha \log P_{L M}(h, t \mid p)+(1-\alpha) \min \left\{\log P_{L M}(h \mid p), \log P_{L M}(t \mid p, h)\right\}
\]</span></p><p>With BERT as an example, the first term in the scoring function involves the probability of filling the entity tuple $&lt;h, t>$ into the slots in the prompt $p$. Typically, this joint conditional probability can be decomposed to $P_{LM}(h|p) \times P_{LM}(t|h,p)$, assuming it’s computed in an autoregressive style. Besides, we also want to make sure that the probability of each step shouldn’t be too low, and that’s the intuition behind the second term. A concrete example is shown in Figure 2, where p=“A is the place for B”, h=“library” and t=“study”. We also present how to process multi-token entities where h=“study room”.</p><p><img src=/bertnet_2.png style=width:6.5in;height:2.65278in></p><p>With this compatibility score, we then introduce the workflow of harvesting knowledge graphs from language models (Figure 3), which can be divided into two main stages: Prompt Creation and Entity Tuple Search.</p><h2 id=stage-1-prompt-creation>Stage 1: Prompt Creation</h2><p><img src=/bertnet_3.png style=width:6.5in;height:3.19444in></p><p>A known deficiency of language models is their inconsistency when given different prompts. Sometimes even a slight difference in wording would cause a drastic change in the prediction results. To this end, we want to generate multiple paraphrases of the initial input prompt, and use them to regularize the output of language models.</p><p>As our implementation, the algorithm iteratively samples entity tuples and prompts to assemble a statement and paraphrase it (specifically usingGPT-3 API). The process is shown in the left part of Figure 3. The generated prompts can be semantically drifted, so the prompts are weighted by the average compatibility scores between a prompt and all given seed entity tuples. The weights are further normalized with softmax across all prompts. The resulting weighted prompt set serves as a more reliable description of the relation.</p><h2 id=stage-2-entity-tuple-search>Stage 2: Entity Tuple Search</h2><p>Our goal in the following stage is to search for entity tuples that achieves high compatibility with the weighted prompt set.</p><p><span class="math display">\[
\text{consistency} \left(\left\langle h^{\text {new }}, t^{\text {new }}\right\rangle\right)=\sum_p w_p \cdot f_{L M}\left(\left\langle h^{\text {new }}, t^{\text {new }}\right\rangle, p\right)
\]</span></p><p>Since the entity search space is too large, we propose an approximation that only uses the minimum individual log-likelihoods (the left part of compatibility score, shortened as MLL) instead of the full equation. This cheaper scoring function allows fast rollouts by pruning.</p><p>As a running example, when we are searching for 100 entity tuples, we maintain a minimum heap to keep track of the MLL of the existing entity pair set. The maximum size of this heap is 100, and the heap top can be used as a threshold for future search because it’s the 100-th largest MLL: When we are searching for a new entity tuple, once we find the log-likelihood at any timestamp is lower than the threshold, we can prune the continuous searching immediately because this means the MLL of this tuple will never surpass any existing tuples in the heap. If a new entity tuple is reached without being pruned, we pop the heap and push the MLL of the new tuple. Intuitively, the pruning process makes sure that the generated part of the tuple in searching is reasonable for the given prompt.</p><p>Once we collect a large number of potential entity tuples, we re-rank them with the full compatibility score. We finally use various thresholds to get the outcome KGs in different scales, including (1) 50%: taking half of all searched-out entities with higher scores. (2) base-k: Naturally there are different numbers of valid tuples for different relations (e.g. tuples of CAPITAL_OF should not exceed 200 as that is the number of all the countries in the world). We design a relation-specific thresholding method, that is to set 10% of the k-nd score as the threshold (i.e., 0.1 ∗ score_k), and retain all tuples with scores above the threshold. We name the settings base-10 and base-100 when k is 10 and 100, respectively.</p><h1 id=outcome-kgs>Outcome KGs</h1><p>Different from traditional KGs, BertNet is extensible in case a new query is desired.. In essence, there is no limitation on the scalability of BertNet. In our evaluation, we ground our framework to a commonly used relation set from ConceptNet, and a New relation set composed of some novel relations the authors think of.</p><p><span class="math display">\[
\begin{array}{lllcc}
\hline \text { Method } & \text { Tuple } & \text { Diversity } & \text { Novelty\% } & \text { Acc\% } \\
\hline \text { WebChild } & 4,649,471 & - & - & 82.0^* \\
\text { ASCENT } & 8,600,000 & - & - & 79.2^* \\
\text { TransOMCS } & 18,481,607 & 100,659 & 98.3 & 56.0^* \\
\hline \text { COMET }_{\text {base-10 }}^{C N} & 6,741 & 4,342 & 35.5 & 92.0 \\
\text { COMET }_{50 \%}^{C N} & 230,028 & 55,350 & 72.4 & 66.6 \\
\hline \text { ROBERTANET }_{\text {base-10 }}^{C N} & 6,741 & 6,107 & 64.4 & 88.0 \\
\text { RoBERTANET }_{\text {base-100 }}^{C N} & 24,375 & 12,762 & 68.8 & 81.6 \\
\text { ROBERTANET }_{50 \%}^{C N} & 230,028 & 80,525 & 87.0 & 55.0 \\
\hline \text { RoBERTANET T }_{\text {base-10 }}^{\text {New }} & 2,180 & 3,137 & - & 81.8 \\
\text { RoBERTANET }_{\text {base-100 }}^{\text {New }} & 7,329 & 6,559 & - & 68.6 \\
\text { ROBERTANET }_{50 \%}^{\text {New }} & 23,666 & 16,089 & - & 58.6 \\
\hline
\end{array}
\]</span></p><p>Using solely the LM as the knowledge source, and without any training data, our framework extracts KGs with high accuracy and diversity. (Other methods build knowledge graphs in different settings, and thus are not comparable to ours) We also show the trade-off between scale and accuracy by using different thresholds.</p><h1 id=analysis-of-our-framework>Analysis of our framework</h1><p>We did another set of experiments to evaluate our prompt creation method. Looking at the top-100 tuples of New relations harvested with different prompts, we found that the paraphrasing-based method outperformed the previous few-shot learning method Autoprompt by a large margin. We also show that utilizing an ensemble of multiple prompts performs better than only using human-written prompts or the top-1 prompts.</p><p>We also harvested KGs from 5 different LMs and evaluated them in the same setting. The results shed some new light on several questions regarding the LMs’ knowledge capacity:</p><p><span class="math display">\[
\begin{array}{rcc}
\hline \text { Methods } & \text { Acc } & \text { Rej } \\
\hline \text { AUTOPROMPT } & 0.33 & 0.47 \\
\text { HUMAN PROMPT } & 0.60 & 0.27 \\
\text { TOP-1 PROMPT (Ours) } & 0.69 & 0.23 \\
\text { MULTI PROMPTS (Ours) } & \mathbf{0 . 7 3} & \mathbf{0 . 2 0} \\
\hline
\end{array}\\
\]</span> <span class="math display">\[
\begin{array}{rcc}
\hline \text { Source LMs } & \text { Acc } & \text { Rej } \\
\hline \text { DISTILBERT } & 0.67 & 0.24 \\
\text { BERT-BASE } & 0.63 & 0.26 \\
\text { BERT-LARGE } & 0.70 & 0.22 \\
\text { ROBERTA-BASE } & 0.70 & 0.22 \\
\text { ROBERTA-LARGE } & 0.73 & 0.20 \\
\hline
\end{array}
\]</span></p><p><strong>Does a larger LM encode better knowledge?</strong></p><p>For BERT (and RoBERTa), the large version and the base version share the same pretraining corpus and tasks, respectively, while the large version has a larger model architecture than the base version in terms of layers (24 v.s. 12), attention heads (16 v.s. 12), and the number of parameters (340M v.s. 110M). We can see that BertNet-large and RoBERTaNet-large are around 7% and 3% higher than their base version, respectively, so the large models indeed encode better knowledge than the base models.</p><p><strong>Does better pretraining bring better knowledge?</strong></p><p>RoBERTa uses the same architecture as BERT but with better pretraining strategies, like dynamic masking, larger batch size, etc. In their extracted knowledge graphs from our framework, RoBERTaNet-large performs better than BertNet-large (0.73 v.s. 0.70), and RoBERTaNet-base is also better than BertNet-base (0.70 v.s. 0.63). This indicates that better pretraining strategies indeed bring better knowledge learning and storage.</p><p><strong>Is knowledge really kept in the knowledge distillation process?</strong></p><p>DistilBERT is trained by distilling BERT-base reducing 40% parameters from it. Interestingly, the knowledge distillation process improves the harvested knowledge graph accuracy by around 4%.. This might be because the knowledge distillation is able to remove some noisy information from the teacher model.</p><h1 id=summary>Summary</h1><p>In this work, we present an automatic framework for extracting a knowledge graph (KG) with arbitrary relations in an efficient and scalable manner, and demonstrate its application in creating KGs from a variety of popular large language models (LMs). Our results indicate that LMs can be effective resources for KG construction on their own. Our framework represents a departure from traditional approaches and offers a fully symbolic interpretation of the LM, providing new insights into its knowledge capabilities.</p></article></div><footer class=markdown>© <a href=mixlabweb.github.io>Mix Lab</a> 2019 – 2022 | <a href=https://creativecommons.org/licenses/by-nc/4.0/>CC BY-NC 4.0</a> | <a href=index.xml>Subscribe</a></footer></body><script>const switchTheme=document.getElementById("switch-theme");switchTheme.addEventListener("click",()=>{document.body.classList.toggle("dark-theme"),document.body.classList.toggle("light-theme"),document.body.classList.contains("dark-theme")?localStorage.setItem("theme","dark-theme"):localStorage.setItem("theme","light-theme")}),tocbot.init({tocSelector:".toc",contentSelector:".markdown",headingSelector:"h1, h2, h3",hasInnerContainers:!0})</script></html>