<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Mix Lab</title><link>https://mixlabweb.github.io/post/</link><description>Recent content in Posts on Mix Lab</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 13 Dec 2022 11:15:33 -0700</lastBuildDate><atom:link href="https://mixlabweb.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models</title><link>https://mixlabweb.github.io/post/2022/12/13/harvesting-symbolic-knowledge-graphs-from-neural-pretrained-language-models/</link><pubDate>Tue, 13 Dec 2022 11:15:33 -0700</pubDate><guid>https://mixlabweb.github.io/post/2022/12/13/harvesting-symbolic-knowledge-graphs-from-neural-pretrained-language-models/</guid><description>Knowledge graphs (KGs) are a useful tool for representing rich symbolic knowledge about entities and their relationships, and have been applied in a variety of fields, including search engines, recommendation systems, chatbots, and healthcare. Traditionally, KGs have been constructed through expensive human crowdsourcing efforts (such as WordNet, ConceptNet, and ATOMIC). Researchers have also explored the use of text mining techniques for automatic KG construction, but this remains a challenging task due to large corpus and complex processing required.</description></item></channel></rss>