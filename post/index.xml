<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Mix Lab</title><link>https://mixlabweb.github.io/post/</link><description>Recent content in Posts on Mix Lab</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 13 Dec 2022 11:15:33 -0700</lastBuildDate><atom:link href="https://mixlabweb.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Harvesting Symbolic Knowledge Graphs from Neural Pretrained Language Models</title><link>https://mixlabweb.github.io/post/2022/12/13/harvesting-symbolic-knowledge-graphs-from-neural-pretrained-language-models/</link><pubDate>Tue, 13 Dec 2022 11:15:33 -0700</pubDate><guid>https://mixlabweb.github.io/post/2022/12/13/harvesting-symbolic-knowledge-graphs-from-neural-pretrained-language-models/</guid><description>There has been a significant increase in the number of deep neural models that have achieved high levels of performance on various tasks across diverse domains, such as language generation with GPT-3 and ChatGPT, and medical prediction with bioBERT. These models have the ability to function as aggregators of information and organizers of diverse data examples and experiences due to the vast amount of knowledge and information that has been implicitly encoded within their parameters.</description></item></channel></rss>